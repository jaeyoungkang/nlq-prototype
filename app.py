import os
import json
import time
import datetime
import logging
from typing import List, Dict, Optional, Generator
from dotenv import load_dotenv
from flask import Flask, Response, render_template, request, jsonify, send_from_directory
from flask_cors import CORS

import anthropic
from google.cloud import bigquery
from google.cloud.exceptions import NotFound, BadRequest

# ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì„í¬íŠ¸
from firestore_db import db_manager

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ ì„í¬íŠ¸ (ëˆ„ë½ëœ ë¶€ë¶„ ì¶”ê°€)
from utils.bigquery_utils import validate_table_ids
from utils.data_utils import (
    safe_json_serialize, 
    suggest_chart_config, 
    analyze_data_structure,
    generate_summary_insights
)

# config íŒ¨í‚¤ì§€ì—ì„œ í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
from config.prompts import (
    get_sql_generation_system_prompt,
    get_analysis_report_prompt,
    get_html_generation_prompt,
    get_profiling_system_prompt
)

# ìŠ¤í‚¤ë§ˆ ê´€ë¦¬ì ì„í¬íŠ¸
from config.schema_config import register_extracted_metadata

# --- ì„¤ì • ë° ë¡œê¹… ---

# .env.local íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv('.env.local')

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸°
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')
if not ANTHROPIC_API_KEY:
    print("ê²½ê³ : ANTHROPIC_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Flask ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
app = Flask(__name__)
CORS(app, origins=["http://localhost:3000"])  # í”„ë¡ íŠ¸ì—”ë“œ URL í—ˆìš©

# --- ê¸€ë¡œë²Œ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---

def initialize_anthropic_client() -> Optional[anthropic.Anthropic]:
    """Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    try:
        if not ANTHROPIC_API_KEY:
            logger.warning("Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
        logger.info("Anthropic í´ë¼ì´ì–¸íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return client
    except Exception as e:
        logger.error(f"Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

def initialize_bigquery_client() -> Optional[bigquery.Client]:
    """BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    try:
        client = bigquery.Client()
        logger.info(f"BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (í”„ë¡œì íŠ¸: {client.project})")
        return client
    except Exception as e:
        logger.error(f"BigQuery í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

# ê¸€ë¡œë²Œ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
anthropic_client = initialize_anthropic_client()
bigquery_client = initialize_bigquery_client()

# --- ì½”ì–´ ë¶„ì„ í´ë˜ìŠ¤ë“¤ ---

class BigQueryMetadataExtractor:
    """BigQuery ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°"""
    
    def __init__(self, bigquery_client: bigquery.Client):
        self.client = bigquery_client
    
    def extract_metadata(self, project_id: str, table_ids: List[str]) -> Dict:
        """í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        metadata = {
            "project_id": project_id,
            "tables": {},
            "summary": {
                "total_tables": len(table_ids),
                "total_rows": 0,
                "total_size_bytes": 0
            },
            "extracted_at": datetime.datetime.now().isoformat()
        }
        
        for table_id in table_ids:
            try:
                table = self.client.get_table(table_id)
                table_info = {
                    "table_id": table_id,
                    "num_rows": table.num_rows,
                    "num_bytes": table.num_bytes,
                    "created": table.created.isoformat() if table.created else None,
                    "modified": table.modified.isoformat() if table.modified else None,
                    "description": table.description or "",
                    "schema": [
                        {
                            "name": field.name,
                            "type": field.field_type,
                            "mode": field.mode,
                            "description": field.description or ""
                        }
                        for field in table.schema
                    ]
                }
                
                # íŒŒí‹°ì…”ë‹ ì •ë³´ ì¶”ê°€
                if table.time_partitioning:
                    table_info["partitioning"] = {
                        "type": table.time_partitioning.type_,
                        "field": table.time_partitioning.field
                    }
                
                # í´ëŸ¬ìŠ¤í„°ë§ ì •ë³´ ì¶”ê°€
                if table.clustering_fields:
                    table_info["clustering"] = {
                        "fields": list(table.clustering_fields)
                    }
                
                metadata["tables"][table_id] = table_info
                metadata["summary"]["total_rows"] += table.num_rows or 0
                metadata["summary"]["total_size_bytes"] += table.num_bytes or 0
                
            except NotFound:
                metadata["tables"][table_id] = {"error": "í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            except Exception as e:
                metadata["tables"][table_id] = {"error": str(e)}
        
        return metadata

class IntegratedAnalyzer:
    """í†µí•© ë¶„ì„ ì—”ì§„"""
    
    def __init__(self, anthropic_client: anthropic.Anthropic, bigquery_client: bigquery.Client):
        self.anthropic_client = anthropic_client
        self.bigquery_client = bigquery_client
        self.metadata_extractor = BigQueryMetadataExtractor(bigquery_client)
    
    def natural_language_to_sql(self, question: str, project_id: str, table_ids: List[str]) -> str:
        """ìì—°ì–´ ì§ˆë¬¸ì„ BigQuery SQLë¡œ ë³€í™˜"""
        if not self.anthropic_client:
            raise Exception("Anthropic í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë™ì  ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = get_sql_generation_system_prompt(project_id, table_ids)
        
        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1500,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": question}
                ]
            )
            
            sql_query = response.content[0].text.strip()
            
            # SQL ì¿¼ë¦¬ì—ì„œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
            sql_query = self._clean_sql_query(sql_query)
            
            logger.info(f"ìƒì„±ëœ SQL: {sql_query}")
            return sql_query
            
        except Exception as e:
            raise Exception(f"Claude API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def _clean_sql_query(self, sql_query: str) -> str:
        """SQL ì¿¼ë¦¬ì—ì„œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±° ë° ì •ë¦¬"""
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        if '```sql' in sql_query:
            # ```sqlê³¼ ```ë¥¼ ì œê±°
            sql_query = sql_query.split('```sql')[1] if '```sql' in sql_query else sql_query
            sql_query = sql_query.split('```')[0] if '```' in sql_query else sql_query
        elif '```' in sql_query:
            # ì¼ë°˜ ì½”ë“œ ë¸”ë¡ ì œê±°
            parts = sql_query.split('```')
            if len(parts) >= 3:
                sql_query = parts[1]  # ì½”ë“œ ë¸”ë¡ ë‚´ìš©ë§Œ ì¶”ì¶œ
        
        # ì•ë’¤ ê³µë°± ì œê±°
        sql_query = sql_query.strip()
        
        # ì£¼ì„ì´ë‚˜ ì„¤ëª… ì œê±° (-- ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ë“¤ ì¤‘ SQL í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²ƒë“¤)
        lines = sql_query.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            # SQL ì£¼ì„ì´ì§€ë§Œ ì‹¤ì œ SQL êµ¬ë¬¸ì´ í¬í•¨ëœ ê²½ìš°ëŠ” ìœ ì§€
            if line.startswith('--') and not any(keyword in line.upper() for keyword in ['SELECT', 'FROM', 'WHERE', 'GROUP', 'ORDER', 'LIMIT']):
                continue
            cleaned_lines.append(line)
        
        # ì •ë¦¬ëœ ë¼ì¸ë“¤ì„ ë‹¤ì‹œ ì¡°í•©
        sql_query = '\n'.join(cleaned_lines)
        
        # ì„¸ë¯¸ì½œë¡ ì´ ì—†ìœ¼ë©´ ì¶”ê°€
        if not sql_query.rstrip().endswith(';'):
            sql_query = sql_query.rstrip() + ';'
        
        return sql_query
    
    def execute_bigquery(self, sql_query: str) -> Dict:
        """BigQueryì—ì„œ SQL ì¿¼ë¦¬ ì‹¤í–‰"""
        try:
            logger.info(f"ì‹¤í–‰í•  SQL: {sql_query}")
            
            query_job = self.bigquery_client.query(sql_query)
            results = query_job.result()
            
            rows = []
            for row in results:
                row_dict = {}
                try:
                    if hasattr(row, 'keys') and hasattr(row, 'values'):
                        for key, value in zip(row.keys(), row.values()):
                            if isinstance(value, datetime.datetime):
                                row_dict[key] = value.isoformat()
                            elif hasattr(value, 'isoformat'):
                                row_dict[key] = value.isoformat()
                            else:
                                row_dict[key] = value
                    else:
                        row_dict = dict(row)
                        for key, value in row_dict.items():
                            if isinstance(value, datetime.datetime):
                                row_dict[key] = value.isoformat()
                            elif hasattr(value, 'isoformat'):
                                row_dict[key] = value.isoformat()
                except Exception as e:
                    logger.error(f"Row ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
                    row_dict = {"error": f"Row ë³€í™˜ ì‹¤íŒ¨: {str(e)}"}
                
                rows.append(row_dict)
            
            # ì•ˆì „í•œ job_stats ìƒì„± (ì†ì„±ì´ ì—†ì„ ê²½ìš° None ì²˜ë¦¬)
            job_stats = {}
            try:
                job_stats["bytes_processed"] = getattr(query_job, 'total_bytes_processed', None)
                job_stats["bytes_billed"] = getattr(query_job, 'total_bytes_billed', None)
                
                # creation_time ì†ì„± ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                creation_time = getattr(query_job, 'creation_time', None) or getattr(query_job, 'created', None)
                if creation_time and hasattr(creation_time, 'isoformat'):
                    job_stats["creation_time"] = creation_time.isoformat()
                else:
                    job_stats["creation_time"] = None
                
                # end_time ì†ì„± ì•ˆì „í•˜ê²Œ ì²˜ë¦¬  
                end_time = getattr(query_job, 'end_time', None) or getattr(query_job, 'ended', None)
                if end_time and hasattr(end_time, 'isoformat'):
                    job_stats["end_time"] = end_time.isoformat()
                else:
                    job_stats["end_time"] = None
                    
                # job_id ì¶”ê°€
                job_stats["job_id"] = getattr(query_job, 'job_id', None)
                
            except Exception as e:
                logger.warning(f"Job stats ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                job_stats = {
                    "bytes_processed": None,
                    "bytes_billed": None,
                    "creation_time": None,
                    "end_time": None,
                    "job_id": None
                }
            
            return {
                "success": True,
                "data": rows,
                "row_count": len(rows),
                "job_stats": job_stats
            }
            
        except Exception as e:
            logger.error(f"BigQuery ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "data": [],
                "error_type": "execution_error"
            }
    
    def generate_analysis_report(self, question: str, sql_query: str, query_results: List[Dict]) -> Dict:
        """êµ¬ì¡°í™”ëœ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.anthropic_client:
            raise Exception("Anthropic í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if not query_results:
            return {
                "report": "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
                "chart_config": None,
                "data_summary": None,
                "insights": []
            }
        
        # ë°ì´í„° êµ¬ì¡° ë¶„ì„
        data_analysis = analyze_data_structure(query_results)
        summary_insights = generate_summary_insights(data_analysis, question)
        
        # ì°¨íŠ¸ ì„¤ì • ì œì•ˆ
        columns = list(query_results[0].keys()) if query_results else []
        chart_config = suggest_chart_config(query_results, columns)
        
        # ë°ì´í„° ìš”ì•½ ìƒì„±
        data_summary = {
            "overview": {
                "total_rows": len(query_results),
                "columns_count": len(columns),
                "data_types": {col: stats["type"] for col, stats in data_analysis["columns"].items()},
                "data_quality_score": data_analysis.get("data_quality", {}).get("overall_score", 0)
            },
            "key_statistics": data_analysis["columns"],
            "quick_insights": summary_insights
        }
        
        # Claudeë¥¼ ì‚¬ìš©í•œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        analysis_prompt = get_analysis_report_prompt(
            question, sql_query, data_analysis, summary_insights, query_results
        )
        
        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=3000,
                messages=[
                    {"role": "user", "content": analysis_prompt}
                ]
            )
            
            analysis_report = response.content[0].text.strip()
            
            return {
                "report": analysis_report,
                "chart_config": chart_config,
                "data_summary": data_summary,
                "insights": summary_insights,
                "data_analysis": data_analysis
            }
            
        except Exception as e:
            raise Exception(f"ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def generate_html_report(self, question: str, sql_query: str, query_results: List[Dict]) -> Dict:
        """ì°½ì˜ì  HTML ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.anthropic_client:
            raise Exception("Anthropic í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if not query_results:
            return {
                "html_content": self._generate_fallback_html(question, []),
                "quality_score": 60,
                "attempts": 1,
                "fallback": True
            }
        
        # HTML ìƒì„± í”„ë¡¬í”„íŠ¸
        html_prompt = get_html_generation_prompt(question, sql_query, query_results)
        
        max_attempts = 2
        for attempt in range(max_attempts):
            try:
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=4000,
                    messages=[
                        {"role": "user", "content": html_prompt}
                    ]
                )
                
                html_content = response.content[0].text.strip()
                
                # HTML ì •ë¦¬
                if not html_content.startswith('<!DOCTYPE') and not html_content.startswith('<html'):
                    if '```html' in html_content:
                        html_content = html_content.split('```html')[1].split('```')[0].strip()
                    elif '```' in html_content:
                        html_content = html_content.split('```')[1].strip()
                
                # ê¸°ë³¸ í’ˆì§ˆ ê²€ì¦
                quality_score = self._validate_html_quality(html_content)
                
                if quality_score >= 70:
                    return {
                        "html_content": html_content,
                        "quality_score": quality_score,
                        "attempts": attempt + 1,
                        "fallback": False
                    }
                
                if attempt < max_attempts - 1:
                    logger.info(f"HTML í’ˆì§ˆ ê°œì„  í•„ìš” (ì ìˆ˜: {quality_score}), ì¬ì‹œë„ ì¤‘...")
                
            except Exception as e:
                logger.error(f"HTML ìƒì„± ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {str(e)}")
        
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ í´ë°±
        return {
            "html_content": self._generate_fallback_html(question, query_results),
            "quality_score": 60,
            "attempts": max_attempts,
            "fallback": True
        }
    
    def _validate_html_quality(self, html_content: str) -> int:
        """HTML í’ˆì§ˆ ê°„ë‹¨ ê²€ì¦"""
        score = 100
        
        if not html_content.startswith('<!DOCTYPE'):
            score -= 20
        if 'Chart.js' in html_content and 'cdnjs.cloudflare.com' not in html_content:
            score -= 20
        if 'new Chart(' not in html_content:
            score -= 15
        if '<style>' not in html_content:
            score -= 15
        
        return max(0, score)
    
    def _generate_fallback_html(self, question: str, query_results: List[Dict]) -> str:
        """í´ë°± HTML ìƒì„±"""
        return f"""<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{question} - ë¶„ì„ ê²°ê³¼</title>
    <style>
        body {{ font-family: 'Segoe UI', sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }}
        .container {{ max-width: 800px; margin: 0 auto; background: white; border-radius: 12px; padding: 30px; box-shadow: 0 4px 12px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 30px; color: #333; }}
        .data-table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        .data-table th {{ background: #4285f4; color: white; padding: 12px; text-align: left; }}
        .data-table td {{ padding: 10px; border-bottom: 1px solid #ddd; }}
        .summary {{ background: #f8f9fa; padding: 15px; border-radius: 8px; margin: 20px 0; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š {question}</h1>
            <p>BigQuery ë¶„ì„ ê²°ê³¼ â€¢ {len(query_results)}ê°œ ê²°ê³¼</p>
        </div>
        <div class="summary">
            <h3>ğŸ“‹ ê¸°ë³¸ ë¶„ì„ ë¦¬í¬íŠ¸</h3>
            <p>ì´ {len(query_results)}ê°œì˜ ë ˆì½”ë“œê°€ ì¡°íšŒë˜ì—ˆìŠµë‹ˆë‹¤.</p>
        </div>
    </div>
</body>
</html>"""

# í†µí•© ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
integrated_analyzer = IntegratedAnalyzer(anthropic_client, bigquery_client) if (anthropic_client and bigquery_client) else None

# --- ë¼ìš°íŠ¸ ì •ì˜ ---

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index.html')

@app.route('/<path:filename>')
def static_files(filename):
    """ì •ì  íŒŒì¼ ì„œë¹™"""
    return send_from_directory('.', filename)

@app.route('/profiling')
def run_profiling():
    """ë©”íƒ€ë°ì´í„° í”„ë¡œíŒŒì¼ë§ (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)"""
    if not integrated_analyzer:
        def error_generator():
            yield f"data: {json.dumps({'type': 'error', 'payload': {'message': 'ë¶„ì„ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}}, ensure_ascii=False)}\n\n"
        return Response(error_generator(), mimetype='text/event-stream')
    
    project_id = request.args.get('projectId', '').strip()
    table_ids_str = request.args.get('tableIds', '').strip()
    
    if not project_id or not table_ids_str:
        def error_generator():
            yield f"data: {json.dumps({'type': 'error', 'payload': {'message': 'Project IDì™€ Table IDsê°€ í•„ìš”í•©ë‹ˆë‹¤.'}}, ensure_ascii=False)}\n\n"
        return Response(error_generator(), mimetype='text/event-stream')
    
    # í…Œì´ë¸” ID íŒŒì‹± ë° ê²€ì¦
    table_ids = [tid.strip() for tid in table_ids_str.replace('\n', ',').split(',') if tid.strip()]
    validated_table_ids = validate_table_ids(table_ids)
    
    if not validated_table_ids:
        def error_generator():
            yield f"data: {json.dumps({'type': 'error', 'payload': {'message': 'ìœ íš¨í•œ í…Œì´ë¸” IDê°€ ì—†ìŠµë‹ˆë‹¤.'}}, ensure_ascii=False)}\n\n"
        return Response(error_generator(), mimetype='text/event-stream')
    
    def profiling_generator():
        try:
            # ì„¸ì…˜ ìƒì„±
            session_data = {
                "id": str(int(time.time() * 1000)),
                "start_time": datetime.datetime.now().isoformat(),
                "project_id": project_id,
                "table_ids": validated_table_ids,
                "status": "ì§„í–‰ ì¤‘"
            }
            session_id = db_manager.create_analysis_session(session_data)
            
            yield f"data: {json.dumps({'type': 'status', 'payload': {'step': 0, 'message': 'ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘...', 'session_id': session_id}}, ensure_ascii=False)}\n\n"
            
            # 1ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            yield f"data: {json.dumps({'type': 'log', 'payload': {'message': f'ëŒ€ìƒ í…Œì´ë¸” {len(validated_table_ids)}ê°œ ë¶„ì„ ì‹œì‘'}}, ensure_ascii=False)}\n\n"
            
            metadata = integrated_analyzer.metadata_extractor.extract_metadata(project_id, validated_table_ids)
            
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ ë“±ë¡
            register_extracted_metadata(project_id, metadata)
            
            yield f"data: {json.dumps({'type': 'status', 'payload': {'step': 1, 'message': 'ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ'}}, ensure_ascii=False)}\n\n"
            yield f"data: {json.dumps({'type': 'metadata', 'payload': safe_json_serialize(metadata)}, ensure_ascii=False)}\n\n"
            
            # 2ë‹¨ê³„: í”„ë¡œíŒŒì¼ë§ ë¦¬í¬íŠ¸ ìƒì„±
            yield f"data: {json.dumps({'type': 'status', 'payload': {'step': 2, 'message': 'ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...'}}, ensure_ascii=False)}\n\n"
            
            # í”„ë¡œíŒŒì¼ë§ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            profiling_prompt = get_profiling_system_prompt()
            
            # ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡œíŒŒì¼ë§ ìˆ˜í–‰
            metadata_summary = f"""
ë‹¤ìŒì€ ì¶”ì¶œëœ BigQuery í…Œì´ë¸” ë©”íƒ€ë°ì´í„°ì…ë‹ˆë‹¤:

í”„ë¡œì íŠ¸ ID: {project_id}
ë¶„ì„ ëŒ€ìƒ í…Œì´ë¸”: {len(validated_table_ids)}ê°œ

{json.dumps(metadata, indent=2, ensure_ascii=False, default=str)}
"""
            
            # ì„¹ì…˜ë³„ í”„ë¡œíŒŒì¼ë§ ìˆ˜í–‰
            sections = [
                ("overview", "ë°ì´í„°ì…‹ ê°œìš” ë¶„ì„ ì¤‘...", "ê°œìš”"),
                ("table_analysis", "í…Œì´ë¸” ìƒì„¸ ë¶„ì„ ì¤‘...", "í…Œì´ë¸” ìƒì„¸ ë¶„ì„"),
                ("relationships", "í…Œì´ë¸” ê´€ê³„ ì¶”ë¡  ì¤‘...", "í…Œì´ë¸” ê°„ ê´€ê³„"),
                ("business_questions", "ë¹„ì¦ˆë‹ˆìŠ¤ ì§ˆë¬¸ ë„ì¶œ ì¤‘...", "ë¶„ì„ ê°€ëŠ¥ ì§ˆë¬¸"),
                ("recommendations", "í™œìš© ê¶Œì¥ì‚¬í•­ ë„ì¶œ ì¤‘...", "ê¶Œì¥ì‚¬í•­")
            ]
            
            profiling_report = {
                "sections": {},
                "full_report": "",
                "generated_at": datetime.datetime.now().isoformat()
            }
            
            for section_key, section_message, section_title in sections:
                yield f"data: {json.dumps({'type': 'log', 'payload': {'message': section_message}}, ensure_ascii=False)}\n\n"
                
                section_prompt = f"{profiling_prompt}\n\n{metadata_summary}\n\nìœ„ ë©”íƒ€ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ '{section_title}' ì„¹ì…˜ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."
                
                try:
                    response = integrated_analyzer.anthropic_client.messages.create(
                        model="claude-3-5-sonnet-20241022",
                        max_tokens=2000,
                        messages=[
                            {"role": "user", "content": section_prompt}
                        ]
                    )
                    
                    section_content = response.content[0].text.strip()
                    profiling_report["sections"][section_key] = section_content
                    
                    # ì„¹ì…˜ë³„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
                    yield f"data: {json.dumps({'type': 'report_section', 'payload': {'section': section_key, 'title': section_title, 'content': section_content}}, ensure_ascii=False)}\n\n"
                    
                    time.sleep(0.2)  # ê° ì„¹ì…˜ ê°„ ì§§ì€ ëŒ€ê¸°
                    
                except Exception as e:
                    yield f"data: {json.dumps({'type': 'log', 'payload': {'message': f'{section_title} ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}'}}, ensure_ascii=False)}\n\n"
                    profiling_report["sections"][section_key] = f"ì„¹ì…˜ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            
            # ì „ì²´ ë¦¬í¬íŠ¸ ì¡°í•©
            full_report_parts = ["# ğŸ“Š BigQuery ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ë¦¬í¬íŠ¸\n"]
            section_titles = {
                "overview": "## 1. ğŸ“‹ ë°ì´í„°ì…‹ ê°œìš”",
                "table_analysis": "## 2. ğŸ” í…Œì´ë¸” ìƒì„¸ ë¶„ì„",
                "relationships": "## 3. ğŸ”— í…Œì´ë¸” ê°„ ê´€ê³„",
                "business_questions": "## 4. â“ ë¶„ì„ ê°€ëŠ¥ ì§ˆë¬¸",
                "recommendations": "## 5. ğŸ’¡ í™œìš© ê¶Œì¥ì‚¬í•­"
            }
            
            for section_key in ["overview", "table_analysis", "relationships", "business_questions", "recommendations"]:
                if section_key in profiling_report["sections"]:
                    full_report_parts.append(f"{section_titles[section_key]}\n{profiling_report['sections'][section_key]}\n")
            
            profiling_report["full_report"] = "\n".join(full_report_parts)
            
            # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
            yield f"data: {json.dumps({'type': 'status', 'payload': {'step': 3, 'message': 'ê²°ê³¼ ì €ì¥ ì¤‘...'}}, ensure_ascii=False)}\n\n"
            
            # Firestoreì— í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì €ì¥
            db_manager.save_analysis_result(session_id, 'profiling_report', profiling_report)
            
            # ì„¸ì…˜ ì™„ë£Œ ì²˜ë¦¬
            db_manager.update_session_status(session_id, "ì™„ë£Œ")
            
            yield f"data: {json.dumps({'type': 'status', 'payload': {'step': 4, 'message': 'í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ', 'session_id': session_id}}, ensure_ascii=False)}\n\n"
            yield f"data: {json.dumps({'type': 'profiling_complete', 'payload': {'session_id': session_id, 'report': profiling_report}}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            logger.error(f"í”„ë¡œíŒŒì¼ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            if 'session_id' in locals():
                db_manager.update_session_status(session_id, "ì‹¤íŒ¨", str(e))
            yield f"data: {json.dumps({'type': 'error', 'payload': {'message': str(e)}}, ensure_ascii=False)}\n\n"
    
    return Response(
        profiling_generator(),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Access-Control-Allow-Origin': '*'
        }
    )

@app.route('/quick', methods=['POST'])
def quick_query():
    """ë¹ ë¥¸ ì¡°íšŒ - ë°ì´í„°ë§Œ ë°˜í™˜"""
    try:
        if not integrated_analyzer:
            return jsonify({
                "success": False,
                "error": "ë¶„ì„ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "mode": "quick"
            }), 500

        if not request.json or 'question' not in request.json:
            return jsonify({
                "success": False,
                "error": "ìš”ì²­ ë³¸ë¬¸ì— 'question' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "mode": "quick"
            }), 400

        question = request.json['question'].strip()
        project_id = request.json.get('project_id', '').strip()
        table_ids = request.json.get('table_ids', [])
        
        if isinstance(table_ids, str):
            table_ids = [tid.strip() for tid in table_ids.replace('\n', ',').split(',') if tid.strip()]
        
        if not question:
            return jsonify({
                "success": False,
                "error": "ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.",
                "mode": "quick"
            }), 400
        
        if not project_id or not table_ids:
            return jsonify({
                "success": False,
                "error": "project_idì™€ table_idsê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "mode": "quick"
            }), 400
        
        # SQL ìƒì„± ë° ë°ì´í„° ì¡°íšŒ
        sql_query = integrated_analyzer.natural_language_to_sql(question, project_id, table_ids)
        query_result = integrated_analyzer.execute_bigquery(sql_query)
        
        if not query_result["success"]:
            return jsonify({
                "success": False,
                "error": query_result["error"],
                "mode": "quick",
                "original_question": question,
                "generated_sql": sql_query,
                "error_type": query_result.get("error_type", "unknown")
            }), 500
        
        return jsonify({
            "success": True,
            "mode": "quick",
            "original_question": question,
            "generated_sql": sql_query,
            "data": query_result["data"],
            "row_count": query_result.get("row_count", 0),
            "execution_stats": query_result.get("job_stats", {})
        })
        
    except Exception as e:
        logger.error(f"ë¹ ë¥¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}",
            "mode": "quick"
        }), 500

@app.route('/analyze', methods=['POST'])
def structured_analysis():
    """êµ¬ì¡°í™”ëœ ë¶„ì„ - ì°¨íŠ¸ì™€ ë¶„ì„ ë¦¬í¬íŠ¸ í¬í•¨"""
    try:
        if not integrated_analyzer:
            return jsonify({
                "success": False,
                "error": "ë¶„ì„ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "mode": "structured"
            }), 500

        if not request.json or 'question' not in request.json:
            return jsonify({
                "success": False,
                "error": "ìš”ì²­ ë³¸ë¬¸ì— 'question' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "mode": "structured"
            }), 400

        question = request.json['question'].strip()
        project_id = request.json.get('project_id', '').strip()
        table_ids = request.json.get('table_ids', [])
        
        if isinstance(table_ids, str):
            table_ids = [tid.strip() for tid in table_ids.replace('\n', ',').split(',') if tid.strip()]
        
        if not question:
            return jsonify({
                "success": False,
                "error": "ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.",
                "mode": "structured"
            }), 400
        
        if not project_id or not table_ids:
            return jsonify({
                "success": False,
                "error": "project_idì™€ table_idsê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "mode": "structured"
            }), 400
        
        # SQL ìƒì„± ë° ë°ì´í„° ì¡°íšŒ
        sql_query = integrated_analyzer.natural_language_to_sql(question, project_id, table_ids)
        query_result = integrated_analyzer.execute_bigquery(sql_query)
        
        if not query_result["success"]:
            return jsonify({
                "success": False,
                "error": query_result["error"],
                "mode": "structured",
                "original_question": question,
                "generated_sql": sql_query,
                "error_type": query_result.get("error_type", "unknown")
            }), 500
        
        # êµ¬ì¡°í™”ëœ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        analysis_result = integrated_analyzer.generate_analysis_report(
            question, 
            sql_query, 
            query_result["data"]
        )
        
        return jsonify({
            "success": True,
            "mode": "structured",
            "original_question": question,
            "generated_sql": sql_query,
            "data": query_result["data"],
            "row_count": query_result.get("row_count", 0),
            "execution_stats": query_result.get("job_stats", {}),
            "analysis_report": analysis_result["report"],
            "chart_config": analysis_result["chart_config"],
            "data_summary": analysis_result["data_summary"],
            "insights": analysis_result["insights"],
            "data_analysis": analysis_result["data_analysis"]
        })
        
    except Exception as e:
        logger.error(f"êµ¬ì¡°í™”ëœ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}",
            "mode": "structured"
        }), 500

@app.route('/creative-html', methods=['POST'])
def creative_html_analysis():
    """ì°½ì˜ì  HTML ë¶„ì„ - Claudeê°€ ì™„ì „í•œ HTML ìƒì„±"""
    try:
        if not integrated_analyzer:
            return jsonify({
                "success": False,
                "error": "ë¶„ì„ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "mode": "creative_html"
            }), 500

        if not request.json or 'question' not in request.json:
            return jsonify({
                "success": False,
                "error": "ìš”ì²­ ë³¸ë¬¸ì— 'question' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "mode": "creative_html"
            }), 400

        question = request.json['question'].strip()
        project_id = request.json.get('project_id', '').strip()
        table_ids = request.json.get('table_ids', [])
        
        if isinstance(table_ids, str):
            table_ids = [tid.strip() for tid in table_ids.replace('\n', ',').split(',') if tid.strip()]
        
        if not question:
            return jsonify({
                "success": False,
                "error": "ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.",
                "mode": "creative_html"
            }), 400
        
        if not project_id or not table_ids:
            return jsonify({
                "success": False,
                "error": "project_idì™€ table_idsê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "mode": "creative_html"
            }), 400
        
        # SQL ìƒì„± ë° ë°ì´í„° ì¡°íšŒ
        sql_query = integrated_analyzer.natural_language_to_sql(question, project_id, table_ids)
        query_result = integrated_analyzer.execute_bigquery(sql_query)
        
        if not query_result["success"]:
            return jsonify({
                "success": False,
                "error": query_result["error"],
                "mode": "creative_html",
                "original_question": question,
                "generated_sql": sql_query,
                "error_type": query_result.get("error_type", "unknown")
            }), 500
        
        # HTML ë¦¬í¬íŠ¸ ìƒì„±
        html_result = integrated_analyzer.generate_html_report(
            question, 
            sql_query, 
            query_result["data"]
        )
        
        return jsonify({
            "success": True,
            "mode": "creative_html",
            "original_question": question,
            "generated_sql": sql_query,
            "row_count": query_result.get("row_count", 0),
            "execution_stats": query_result.get("job_stats", {}),
            "html_content": html_result["html_content"],
            "quality_score": html_result["quality_score"],
            "attempts": html_result["attempts"],
            "is_fallback": html_result.get("fallback", False)
        })
        
    except Exception as e:
        logger.error(f"ì°½ì˜ì  HTML ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}",
            "mode": "creative_html"
        }), 500

# --- ì„¸ì…˜ ê´€ë¦¬ ë¼ìš°íŠ¸ ---

@app.route('/logs')
def get_logs():
    """ì €ì¥ëœ ë¶„ì„ ì‘ì—… ê¸°ë¡ì„ ë°˜í™˜"""
    try:
        page = int(request.args.get('page', 1))
        limit = int(request.args.get('limit', 20))
        project_id = request.args.get('project_id')
        
        logs = db_manager.get_analysis_sessions(limit=limit, project_id=project_id)
        
        logger.info(f"ê¸°ë¡ ì¡°íšŒ: {len(logs)}ê°œ í•­ëª©")
        return jsonify(logs)
        
    except Exception as e:
        logger.error(f"ê¸°ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return jsonify({"error": "ê¸°ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500

@app.route('/logs/<session_id>')
def get_log_detail(session_id):
    """íŠ¹ì • ì„¸ì…˜ì˜ ìƒì„¸ ì •ë³´ì™€ ë¡œê·¸ë¥¼ ë°˜í™˜"""
    try:
        include_logs = request.args.get('include_logs', 'true').lower() == 'true'
        log = db_manager.get_analysis_session_with_logs(session_id, include_logs)
        if not log:
            return jsonify({"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404
        return jsonify(log)
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return jsonify({"error": "ì„¸ì…˜ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500

@app.route('/logs/<session_id>', methods=['DELETE'])
def delete_log(session_id):
    """ë¶„ì„ ì„¸ì…˜ì„ ì‚­ì œ"""
    try:
        success = db_manager.delete_analysis_session(session_id)
        if success:
            return jsonify({"message": "ì„¸ì…˜ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."})
        else:
            return jsonify({"error": "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
        return jsonify({"error": "ì„¸ì…˜ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500

# --- ìœ í‹¸ë¦¬í‹° ë¼ìš°íŠ¸ ---

@app.route('/health', methods=['GET'])
def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.datetime.now().isoformat(),
        "bigquery_client": "configured" if bigquery_client else "not configured",
        "services": {
            "anthropic": "configured" if ANTHROPIC_API_KEY else "not configured",
            "bigquery": "configured" if bigquery_client else "not configured",
            "firestore": "configured" if db_manager.db else "not configured"
        },
        "supported_modes": ["quick", "analyze", "creative_html"],
        "version": "1.0.0-integrated"
    })

@app.route('/stats')
def get_stats():
    """í†µê³„ ì •ë³´ ì¡°íšŒ"""
    try:
        stats = db_manager.get_project_stats()
        return jsonify(stats)
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return jsonify({"error": "í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500

# --- ì˜¤ë¥˜ í•¸ë“¤ëŸ¬ ---

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        "success": False,
        "error": "ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    }), 404

@app.errorhandler(500)
def internal_error(error):
    logger.error(f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {error}")
    return jsonify({
        "success": False,
        "error": "ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    }), 500

if __name__ == '__main__':
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    if not ANTHROPIC_API_KEY:
        logger.warning("ê²½ê³ : ANTHROPIC_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not bigquery_client:
        logger.warning("ê²½ê³ : BigQuery í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not integrated_analyzer:
        logger.warning("ê²½ê³ : í†µí•© ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    logger.info("í†µí•© BigQuery ë¶„ì„ê¸° ì„œë²„ ì‹œì‘")
    logger.info(f"Anthropic API ìƒíƒœ: {'ì‚¬ìš© ê°€ëŠ¥' if anthropic_client else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"BigQuery ìƒíƒœ: {'ì‚¬ìš© ê°€ëŠ¥' if bigquery_client else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info(f"Firestore ìƒíƒœ: {'ì‚¬ìš© ê°€ëŠ¥' if db_manager.db else 'ì‚¬ìš© ë¶ˆê°€'}")
    logger.info("ì§€ì› ëª¨ë“œ: ë¹ ë¥¸ ì¡°íšŒ(/quick), êµ¬ì¡°í™”ëœ ë¶„ì„(/analyze), ì°½ì˜ì  HTML(/creative-html)")
    
    # Cloud Runì—ì„œëŠ” PORT í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
    port = int(os.getenv('PORT', 8080))
    app.run(debug=False, host='0.0.0.0', port=port)  # í”„ë¡œë•ì…˜ì—ì„œëŠ” debug=False