# api/routes.py
"""
API ë¼ìš°íŠ¸ - ë°ì´í„° ë¶„ì„ ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸ (í”„ë¡œíŒŒì¼ë§ í†µí•© ê°œì„ )
"""

import os
import json
import time
import datetime
import logging
from typing import List, Dict, Optional

from flask import Blueprint, request, jsonify, Response
from google.cloud import bigquery

from firestore_db import db_manager
from utils.bigquery_utils import validate_table_ids
from utils.data_utils import safe_json_serialize
from config.schema_config import register_extracted_metadata
from config.prompts import get_profiling_system_prompt

logger = logging.getLogger(__name__)

# Blueprint ìƒì„±
analysis_bp = Blueprint('analysis', __name__)

# ì „ì—­ ë³€ìˆ˜ (app.pyì—ì„œ ì„¤ì •ë¨)
integrated_analyzer = None
bigquery_client = None

def init_routes(analyzer, bq_client):
    """ë¼ìš°íŠ¸ ì´ˆê¸°í™” í•¨ìˆ˜"""
    global integrated_analyzer, bigquery_client
    integrated_analyzer = analyzer
    bigquery_client = bq_client


@analysis_bp.route('/quick', methods=['POST'])
def quick_query():
    """ë¹ ë¥¸ ì¡°íšŒ - ë°ì´í„°ë§Œ ë°˜í™˜"""
    try:
        if not integrated_analyzer:
            return jsonify({
                "success": False, 
                "error": "ë¶„ì„ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 500
            
        data = request.json
        if not data or 'question' not in data:
            return jsonify({
                "success": False, 
                "error": "ìš”ì²­ ë³¸ë¬¸ì— 'question' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤."
            }), 400
        
        question = data['question'].strip()
        project_id = data.get('project_id', '').strip()
        table_ids = data.get('table_ids', [])
        
        if not question or not project_id or not table_ids:
            return jsonify({
                "success": False, 
                "error": "ì§ˆë¬¸, í”„ë¡œì íŠ¸ ID, í…Œì´ë¸” IDê°€ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤."
            }), 400

        # SQL ìƒì„±
        sql_query = integrated_analyzer.natural_language_to_sql(question, project_id, table_ids)
        
        # BigQuery ì‹¤í–‰
        query_result = integrated_analyzer.execute_bigquery(sql_query)
        
        if not query_result["success"]:
            return jsonify({
                "success": False, 
                "error": query_result["error"], 
                "generated_sql": sql_query,
                "error_type": query_result.get("error_type", "unknown")
            }), 500
        
        return jsonify({
            "success": True, 
            "original_question": question, 
            "generated_sql": sql_query,
            "data": query_result["data"], 
            "row_count": query_result.get("row_count", 0),
            "execution_stats": query_result.get("job_stats", {})
        })
        
    except Exception as e:
        logger.error(f"ë¹ ë¥¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "success": False, 
            "error": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}"
        }), 500


@analysis_bp.route('/analyze-context', methods=['POST'])
def analyze_context():
    """ìš”ì²­ëœ íŠ¹ì • ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰"""
    try:
        if not integrated_analyzer:
            return jsonify({
                "success": False, 
                "error": "ë¶„ì„ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 500
            
        data = request.json
        required_fields = ['question', 'sql_query', 'query_results', 'project_id', 'table_ids', 'analysis_type']
        
        if not data or not all(field in data for field in required_fields):
            missing_fields = [field for field in required_fields if field not in (data or {})]
            return jsonify({
                "success": False, 
                "error": f"í•„ìˆ˜ í•„ë“œê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_fields)}"
            }), 400

        # ë¶„ì„ íƒ€ì… ê²€ì¦
        valid_analysis_types = ['explanation', 'context', 'suggestion']
        if data['analysis_type'] not in valid_analysis_types:
            return jsonify({
                "success": False,
                "error": f"ìœ íš¨í•˜ì§€ ì•Šì€ ë¶„ì„ íƒ€ì…ì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ê°’: {', '.join(valid_analysis_types)}"
            }), 400

        # íŠ¹ì • ë¶„ì„ ìˆ˜í–‰
        analysis = integrated_analyzer.generate_specific_analysis(
            question=data['question'], 
            sql_query=data['sql_query'], 
            query_results=data['query_results'],
            project_id=data['project_id'], 
            table_ids=data['table_ids'], 
            analysis_type=data['analysis_type']
        )
        
        return jsonify({
            "success": True, 
            "analysis": analysis,
            "analysis_type": data['analysis_type']
        })
        
    except Exception as e:
        logger.error(f"ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "success": False, 
            "error": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}"
        }), 500


@analysis_bp.route('/profiling')
def run_profiling():
    """í†µí•© í”„ë¡œíŒŒì¼ë§ ì—”ë“œí¬ì¸íŠ¸ (ì„¤ì • í˜ì´ì§€ìš© ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)"""
    if not integrated_analyzer:
        def error_generator():
            error_data = {
                'type': 'error', 
                'payload': {'message': 'ë¶„ì„ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        return Response(error_generator(), mimetype='text/event-stream')
    
    # ìš”ì²­ íŒŒë¼ë¯¸í„° ê²€ì¦ (GET íŒŒë¼ë¯¸í„°ì™€ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ëª¨ë‘ ì§€ì›)
    project_id = request.args.get('projectId', '').strip()
    table_ids_str = request.args.get('tableIds', '').strip()
    
    if not project_id or not table_ids_str:
        def error_generator():
            error_data = {
                'type': 'error', 
                'payload': {'message': 'projectIdì™€ tableIds íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.'}
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        return Response(error_generator(), mimetype='text/event-stream')
    
    # í…Œì´ë¸” ID íŒŒì‹± ë° ê²€ì¦
    table_ids = [tid.strip() for tid in table_ids_str.replace('\n', ',').split(',') if tid.strip()]
    validated_table_ids = validate_table_ids(table_ids)
    
    if not validated_table_ids:
        def error_generator():
            error_data = {
                'type': 'error', 
                'payload': {'message': 'ìœ íš¨í•œ í…Œì´ë¸” IDê°€ ì—†ìŠµë‹ˆë‹¤.'}
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        return Response(error_generator(), mimetype='text/event-stream')
    
    def profiling_generator():
        session_id = None
        try:
            # ì„¸ì…˜ ìƒì„±
            session_data = {
                "id": str(int(time.time() * 1000)),
                "start_time": datetime.datetime.now().isoformat(),
                "project_id": project_id,
                "table_ids": validated_table_ids,
                "status": "ì§„í–‰ ì¤‘"
            }
            session_id = db_manager.create_analysis_session(session_data)
            
            # ì‹œì‘ ìƒíƒœ ì „ì†¡
            yield f"data: {json.dumps({'type': 'status', 'payload': {'step': 0, 'message': 'ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘...', 'session_id': session_id}}, ensure_ascii=False)}\n\n"
            
            # 1ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            yield f"data: {json.dumps({'type': 'log', 'payload': {'message': f'ëŒ€ìƒ í…Œì´ë¸” {len(validated_table_ids)}ê°œ ë¶„ì„ ì‹œì‘'}}, ensure_ascii=False)}\n\n"
            
            metadata = integrated_analyzer.metadata_extractor.extract_metadata(project_id, validated_table_ids)
            
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ ë“±ë¡
            register_extracted_metadata(project_id, metadata)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ ìƒíƒœ
            yield f"data: {json.dumps({'type': 'status', 'payload': {'step': 1, 'message': 'ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ'}}, ensure_ascii=False)}\n\n"
            
            # ë©”íƒ€ë°ì´í„° ì „ì†¡
            yield f"data: {json.dumps({'type': 'metadata', 'payload': safe_json_serialize(metadata)}, ensure_ascii=False)}\n\n"
            
            # 2ë‹¨ê³„: í”„ë¡œíŒŒì¼ë§ ë¦¬í¬íŠ¸ ìƒì„±
            yield f"data: {json.dumps({'type': 'status', 'payload': {'step': 2, 'message': 'ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...'}}, ensure_ascii=False)}\n\n"
            
            # í”„ë¡œíŒŒì¼ë§ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            profiling_prompt = get_profiling_system_prompt()
            
            # ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í”„ë¡œíŒŒì¼ë§ ìˆ˜í–‰
            metadata_summary = f"""
ë‹¤ìŒì€ ì¶”ì¶œëœ BigQuery í…Œì´ë¸” ë©”íƒ€ë°ì´í„°ì…ë‹ˆë‹¤:

í”„ë¡œì íŠ¸ ID: {project_id}
ë¶„ì„ ëŒ€ìƒ í…Œì´ë¸”: {len(validated_table_ids)}ê°œ

{json.dumps(metadata, indent=2, ensure_ascii=False, default=str)}
"""
            
            # ì„¹ì…˜ë³„ í”„ë¡œíŒŒì¼ë§ ìˆ˜í–‰
            sections = [
                ("overview", "ë°ì´í„°ì…‹ ê°œìš” ë¶„ì„ ì¤‘...", "ê°œìš”"),
                ("table_analysis", "í…Œì´ë¸” ìƒì„¸ ë¶„ì„ ì¤‘...", "í…Œì´ë¸” ìƒì„¸ ë¶„ì„"),
                ("relationships", "í…Œì´ë¸” ê´€ê³„ ì¶”ë¡  ì¤‘...", "í…Œì´ë¸” ê°„ ê´€ê³„"),
                ("business_questions", "ë¹„ì¦ˆë‹ˆìŠ¤ ì§ˆë¬¸ ë„ì¶œ ì¤‘...", "ë¶„ì„ ê°€ëŠ¥ ì§ˆë¬¸"),
                ("recommendations", "í™œìš© ê¶Œì¥ì‚¬í•­ ë„ì¶œ ì¤‘...", "ê¶Œì¥ì‚¬í•­")
            ]
            
            profiling_report = {
                "sections": {},
                "full_report": "",
                "generated_at": datetime.datetime.now().isoformat()
            }
            
            for i, (section_key, section_message, section_title) in enumerate(sections):
                # ì„¹ì…˜ ì§„í–‰ ìƒíƒœ ì „ì†¡
                yield f"data: {json.dumps({'type': 'status', 'payload': {'step': 2, 'message': section_message}}, ensure_ascii=False)}\n\n"
                yield f"data: {json.dumps({'type': 'log', 'payload': {'message': section_message}}, ensure_ascii=False)}\n\n"
                
                section_prompt = f"{profiling_prompt}\n\n{metadata_summary}\n\nìœ„ ë©”íƒ€ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ '{section_title}' ì„¹ì…˜ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."
                
                try:
                    response = integrated_analyzer.anthropic_client.messages.create(
                        model="claude-3-5-sonnet-20241022",
                        max_tokens=2000,
                        messages=[
                            {"role": "user", "content": section_prompt}
                        ]
                    )
                    
                    section_content = response.content[0].text.strip()
                    profiling_report["sections"][section_key] = section_content
                    
                    # ì„¹ì…˜ë³„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
                    yield f"data: {json.dumps({'type': 'report_section', 'payload': {'section': section_key, 'title': section_title, 'content': section_content}}, ensure_ascii=False)}\n\n"
                    
                    time.sleep(0.2)  # ê° ì„¹ì…˜ ê°„ ì§§ì€ ëŒ€ê¸°
                    
                except Exception as e:
                    error_message = f'{section_title} ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}'
                    yield f"data: {json.dumps({'type': 'log', 'payload': {'message': error_message}}, ensure_ascii=False)}\n\n"
                    profiling_report["sections"][section_key] = f"ì„¹ì…˜ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            
            # ì „ì²´ ë¦¬í¬íŠ¸ ì¡°í•©
            full_report_parts = ["# ğŸ“Š BigQuery ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ë¦¬í¬íŠ¸\n"]
            section_titles = {
                "overview": "## 1. ğŸ“‹ ë°ì´í„°ì…‹ ê°œìš”",
                "table_analysis": "## 2. ğŸ” í…Œì´ë¸” ìƒì„¸ ë¶„ì„",
                "relationships": "## 3. ğŸ”— í…Œì´ë¸” ê°„ ê´€ê³„",
                "business_questions": "## 4. â“ ë¶„ì„ ê°€ëŠ¥ ì§ˆë¬¸",
                "recommendations": "## 5. ğŸ’¡ í™œìš© ê¶Œì¥ì‚¬í•­"
            }
            
            for section_key in ["overview", "table_analysis", "relationships", "business_questions", "recommendations"]:
                if section_key in profiling_report["sections"]:
                    full_report_parts.append(f"{section_titles[section_key]}\n{profiling_report['sections'][section_key]}\n")
            
            profiling_report["full_report"] = "\n".join(full_report_parts)
            
            # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
            yield f"data: {json.dumps({'type': 'status', 'payload': {'step': 3, 'message': 'ê²°ê³¼ ì €ì¥ ì¤‘...'}}, ensure_ascii=False)}\n\n"
            
            # Firestoreì— í”„ë¡œíŒŒì¼ë§ ê²°ê³¼ ì €ì¥
            db_manager.save_analysis_result(session_id, 'profiling_report', profiling_report)
            
            # ì„¸ì…˜ ì™„ë£Œ ì²˜ë¦¬
            db_manager.update_session_status(session_id, "ì™„ë£Œ")
            
            # ì™„ë£Œ ìƒíƒœ ì „ì†¡
            yield f"data: {json.dumps({'type': 'status', 'payload': {'step': 4, 'message': 'í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ', 'session_id': session_id}}, ensure_ascii=False)}\n\n"
            
            # ìµœì¢… ì™„ë£Œ ë°ì´í„° ì „ì†¡
            yield f"data: {json.dumps({'type': 'profiling_complete', 'payload': {'session_id': session_id, 'report': profiling_report}}, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            logger.error(f"í”„ë¡œíŒŒì¼ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # ì„¸ì…˜ ì‹¤íŒ¨ ì²˜ë¦¬
            if session_id:
                db_manager.update_session_status(session_id, "ì‹¤íŒ¨", str(e))
            
            # ì˜¤ë¥˜ ì „ì†¡
            yield f"data: {json.dumps({'type': 'error', 'payload': {'message': str(e)}}, ensure_ascii=False)}\n\n"
    
    return Response(
        profiling_generator(),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Headers': 'Cache-Control'
        }
    )


@analysis_bp.route('/analyze', methods=['POST'])
def structured_analysis():
    """êµ¬ì¡°í™”ëœ ë¶„ì„ - ì°¨íŠ¸ì™€ ë¶„ì„ ë¦¬í¬íŠ¸ í¬í•¨"""
    try:
        if not integrated_analyzer:
            return jsonify({
                "success": False,
                "error": "ë¶„ì„ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "mode": "structured"
            }), 500

        if not request.json or 'question' not in request.json:
            return jsonify({
                "success": False,
                "error": "ìš”ì²­ ë³¸ë¬¸ì— 'question' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "mode": "structured"
            }), 400

        question = request.json['question'].strip()
        project_id = request.json.get('project_id', '').strip()
        table_ids = request.json.get('table_ids', [])
        
        # í…Œì´ë¸” ID ì •ì œ
        if isinstance(table_ids, str):
            table_ids = [tid.strip() for tid in table_ids.replace('\n', ',').split(',') if tid.strip()]
        
        # ì…ë ¥ ê²€ì¦
        if not question:
            return jsonify({
                "success": False,
                "error": "ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.",
                "mode": "structured"
            }), 400
        
        if not project_id or not table_ids:
            return jsonify({
                "success": False,
                "error": "project_idì™€ table_idsê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "mode": "structured"
            }), 400
        
        # SQL ìƒì„± ë° ë°ì´í„° ì¡°íšŒ
        sql_query = integrated_analyzer.natural_language_to_sql(question, project_id, table_ids)
        query_result = integrated_analyzer.execute_bigquery(sql_query)
        
        if not query_result["success"]:
            return jsonify({
                "success": False,
                "error": query_result["error"],
                "mode": "structured",
                "original_question": question,
                "generated_sql": sql_query,
                "error_type": query_result.get("error_type", "unknown")
            }), 500
        
        # êµ¬ì¡°í™”ëœ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        analysis_result = integrated_analyzer.generate_analysis_report(
            question, 
            sql_query, 
            query_result["data"]
        )
        
        return jsonify({
            "success": True,
            "mode": "structured",
            "original_question": question,
            "generated_sql": sql_query,
            "data": query_result["data"],
            "row_count": query_result.get("row_count", 0),
            "execution_stats": query_result.get("job_stats", {}),
            "analysis_report": analysis_result["report"],
            "chart_config": analysis_result["chart_config"],
            "data_summary": analysis_result["data_summary"],
            "insights": analysis_result["insights"],
            "data_analysis": analysis_result["data_analysis"]
        })
        
    except Exception as e:
        logger.error(f"êµ¬ì¡°í™”ëœ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            "success": False,
            "error": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}",
            "mode": "structured"
        }), 500


@analysis_bp.route('/validate-query', methods=['POST'])
def validate_query():
    """SQL ì¿¼ë¦¬ ë¬¸ë²• ê²€ì¦ (ì‹¤í–‰í•˜ì§€ ì•Šê³  ê²€ì¦ë§Œ)"""
    try:
        if not bigquery_client:
            return jsonify({
                "success": False,
                "error": "BigQuery í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }), 500

        data = request.json
        if not data or 'sql_query' not in data:
            return jsonify({
                "success": False,
                "error": "ìš”ì²­ ë³¸ë¬¸ì— 'sql_query' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤."
            }), 400

        sql_query = data['sql_query'].strip()
        if not sql_query:
            return jsonify({
                "success": False,
                "error": "SQL ì¿¼ë¦¬ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
            }), 400

        # ë“œë¼ì´ ëŸ°ìœ¼ë¡œ ê²€ì¦
        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
        query_job = bigquery_client.query(sql_query, job_config=job_config)

        return jsonify({
            "success": True,
            "valid": True,
            "bytes_processed": query_job.total_bytes_processed,
            "estimated_cost": round(query_job.total_bytes_processed / (1024**4) * 5, 4),  # $5 per TB
            "message": "ì¿¼ë¦¬ê°€ ìœ íš¨í•©ë‹ˆë‹¤."
        })

    except Exception as e:
        return jsonify({
            "success": True,
            "valid": False,
            "error": str(e),
            "message": "ì¿¼ë¦¬ì— ì˜¤ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤."
        })


# ë””ë²„ê¹…ì„ ìœ„í•œ ìƒíƒœ í™•ì¸ ì—”ë“œí¬ì¸íŠ¸
@analysis_bp.route('/status')
def get_analysis_status():
    """ë¶„ì„ ì—”ì§„ ìƒíƒœ í™•ì¸"""
    return jsonify({
        "integrated_analyzer": "initialized" if integrated_analyzer else "not initialized",
        "bigquery_client": "initialized" if bigquery_client else "not initialized",
        "anthropic_client": "initialized" if (integrated_analyzer and integrated_analyzer.anthropic_client) else "not initialized",
        "metadata_extractor": "initialized" if (integrated_analyzer and integrated_analyzer.metadata_extractor) else "not initialized"
    })