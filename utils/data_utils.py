# utils/data_utils.py
"""
í†µí•© ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""

from datetime import datetime
from typing import Dict, List, Any, Optional

def safe_json_serialize(obj):
    """JSON ì§ë ¬í™”ë¥¼ ì•ˆì „í•˜ê²Œ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    try:
        if isinstance(obj, dict):
            return {str(k): safe_json_serialize(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [safe_json_serialize(item) for item in obj]
        elif isinstance(obj, (datetime, )):
            return obj.isoformat()
        elif hasattr(obj, 'isoformat'):
            return obj.isoformat()
        elif isinstance(obj, (int, float, str, bool)) or obj is None:
            return obj
        else:
            return str(obj)
    except Exception as e:
        print(f"JSON ì§ë ¬í™” ì˜¤ë¥˜: {e}")
        return str(obj)

def suggest_chart_config(data: List[Dict], columns: List[str]) -> Optional[Dict]:
    """ë°ì´í„° êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì°¨íŠ¸ ì„¤ì • ì œì•ˆ"""
    if not data or len(data) == 0:
        return None
    
    # ì»¬ëŸ¼ ê°œìˆ˜ì— ë”°ë¥¸ ì°¨íŠ¸ íƒ€ì… ê²°ì •
    if len(columns) == 1:
        return None  # ë‹¨ì¼ ì»¬ëŸ¼ì€ ì°¨íŠ¸ë¡œ í‘œí˜„í•˜ê¸° ì–´ë ¤ì›€
    
    if len(columns) == 2:
        # 2ê°œ ì»¬ëŸ¼ì¸ ê²½ìš°
        col1, col2 = columns[0], columns[1]
        
        # ì²« ë²ˆì§¸ ê°’ìœ¼ë¡œ ë°ì´í„° íƒ€ì… íŒë‹¨
        first_val1 = data[0][col1] if data[0][col1] is not None else ""
        first_val2 = data[0][col2] if data[0][col2] is not None else 0
        
        # ë‘ ë²ˆì§¸ ì»¬ëŸ¼ì´ ìˆ«ìì¸ ê²½ìš°
        if isinstance(first_val2, (int, float)):
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ì¹´í…Œê³ ë¦¬ì¸ ê²½ìš°
            if isinstance(first_val1, str):
                return {
                    "type": "bar",
                    "label_column": col1,
                    "value_column": col2,
                    "title": f"{col1}ë³„ {col2}",
                    "chart_library": "Chart.js"
                }
    
    # 3ê°œ ì´ìƒ ì»¬ëŸ¼ì¸ ê²½ìš° ì²« ë²ˆì§¸ëŠ” ë¼ë²¨, ë‚˜ë¨¸ì§€ëŠ” ê°’ìœ¼ë¡œ ê°€ì •
    if len(columns) >= 3:
        label_col = columns[0]
        value_cols = columns[1:]
        
        # ëª¨ë“  ê°’ ì»¬ëŸ¼ì´ ìˆ«ìì¸ì§€ í™•ì¸
        all_numeric = True
        for col in value_cols:
            if data[0][col] is not None and not isinstance(data[0][col], (int, float)):
                all_numeric = False
                break
        
        if all_numeric:
            return {
                "type": "line" if len(value_cols) > 1 else "bar",
                "label_column": label_col,
                "value_columns": value_cols,
                "title": f"{label_col}ë³„ ë°ì´í„° ë¹„êµ",
                "chart_library": "Chart.js"
            }
    
    return None

def analyze_data_structure(data: List[Dict]) -> Dict:
    """ë°ì´í„° êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ í†µê³„ ìš”ì•½ ìƒì„±"""
    if not data or len(data) == 0:
        return {
            "row_count": 0,
            "columns": {},
            "summary_stats": {},
            "patterns": [],
            "data_quality": {
                "completeness": 0,
                "consistency": 0,
                "overall_score": 0
            }
        }
    
    # ë°ì´í„° íƒ€ì… ê²€ì¦
    if not isinstance(data, list):
        print(f"ê²½ê³ : ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(data)}")
        return {
            "row_count": 0,
            "columns": {},
            "summary_stats": {},
            "patterns": ["ë°ì´í„° íƒ€ì… ì˜¤ë¥˜"],
            "data_quality": {"completeness": 0, "consistency": 0, "overall_score": 0}
        }
    
    # ì²« ë²ˆì§¸ í–‰ ê²€ì¦
    if not data or not isinstance(data[0], dict):
        print(f"ê²½ê³ : ì²« ë²ˆì§¸ í–‰ì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(data[0]) if data else 'None'}")
        return {
            "row_count": len(data),
            "columns": {},
            "summary_stats": {},
            "patterns": ["ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜"],
            "data_quality": {"completeness": 0, "consistency": 0, "overall_score": 0}
        }
    
    analysis = {
        "row_count": len(data),
        "columns": {},
        "summary_stats": {},
        "patterns": [],
        "data_quality": {"completeness": 0, "consistency": 0, "overall_score": 0}
    }
    
    total_completeness = 0
    total_consistency = 0
    
    # ê° ì»¬ëŸ¼ë³„ ë¶„ì„
    try:
        for col in data[0].keys():
            # ì•ˆì „í•œ ê°’ ì¶”ì¶œ
            values = []
            for row in data:
                if isinstance(row, dict) and col in row:
                    val = row[col]
                    if val is not None:
                        values.append(val)
            
            non_null_count = len(values)
            null_count = len(data) - non_null_count
            completeness = (non_null_count / len(data)) * 100 if len(data) > 0 else 0
            
            col_analysis = {
                "type": "unknown",
                "non_null_count": non_null_count,
                "null_count": null_count,
                "null_percentage": round((null_count / len(data)) * 100, 1) if len(data) > 0 else 0,
                "completeness": round(completeness, 1),
                "data_quality_issues": []
            }
            
            # ë°ì´í„° ì¼ê´€ì„± ì²´í¬
            consistency_score = 100
            
            if values:
                # ë°ì´í„° íƒ€ì… íŒë‹¨
                first_val = values[0]
                if isinstance(first_val, (int, float)):
                    col_analysis["type"] = "numeric"
                    try:
                        numeric_values = [float(v) for v in values if isinstance(v, (int, float))]
                        if numeric_values:
                            col_analysis.update({
                                "min": min(numeric_values),
                                "max": max(numeric_values),
                                "mean": round(sum(numeric_values) / len(numeric_values), 2),
                                "median": round(sorted(numeric_values)[len(numeric_values)//2], 2),
                                "sum": sum(numeric_values),
                                "range": max(numeric_values) - min(numeric_values)
                            })
                            
                            # ì´ìƒì¹˜ ê²€ì‚¬ (IQR ë°©ë²•)
                            q1 = sorted(numeric_values)[len(numeric_values)//4]
                            q3 = sorted(numeric_values)[3*len(numeric_values)//4]
                            iqr = q3 - q1
                            outliers = [v for v in numeric_values if v < q1 - 1.5*iqr or v > q3 + 1.5*iqr]
                            if outliers:
                                col_analysis["outliers"] = len(outliers)
                                col_analysis["data_quality_issues"].append(f"{len(outliers)}ê°œì˜ ì´ìƒì¹˜ ë°œê²¬")
                                consistency_score -= min(20, len(outliers) / len(numeric_values) * 100)
                                
                    except Exception as e:
                        print(f"ìˆ«ì ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                        col_analysis["data_quality_issues"].append("ìˆ«ì ë¶„ì„ ì‹¤íŒ¨")
                        consistency_score -= 30
                        
                elif isinstance(first_val, str):
                    col_analysis["type"] = "categorical"
                    try:
                        unique_values = list(set(values))
                        col_analysis.update({
                            "unique_count": len(unique_values),
                            "cardinality": round(len(unique_values) / len(values) * 100, 1),
                            "most_common": max(set(values), key=values.count) if values else None,
                            "top_values": dict(sorted(
                                [(v, values.count(v)) for v in set(values[:100])], # ì„±ëŠ¥ì„ ìœ„í•´ ìƒìœ„ 100ê°œë§Œ ì²˜ë¦¬
                                key=lambda x: x[1], reverse=True
                            )[:5])
                        })
                        
                        # ë°ì´í„° ì¼ê´€ì„± ì²´í¬ (ë¹ˆ ë¬¸ìì—´, ê³µë°± ë“±)
                        empty_strings = sum(1 for v in values if isinstance(v, str) and not v.strip())
                        if empty_strings > 0:
                            col_analysis["data_quality_issues"].append(f"{empty_strings}ê°œì˜ ë¹ˆ ë¬¸ìì—´")
                            consistency_score -= min(15, empty_strings / len(values) * 100)
                            
                        # ë¬¸ìì—´ ê¸¸ì´ ì¼ê´€ì„± ì²´í¬
                        string_lengths = [len(str(v)) for v in values]
                        if string_lengths:
                            length_variance = max(string_lengths) - min(string_lengths)
                            if length_variance > 100:  # ì„ê³„ê°’
                                col_analysis["data_quality_issues"].append("ë¬¸ìì—´ ê¸¸ì´ ë¶ˆì¼ì¹˜")
                                consistency_score -= 10
                                
                    except Exception as e:
                        print(f"ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                        col_analysis["unique_count"] = len(set(str(v) for v in values[:100]))
                        consistency_score -= 20
                        
                elif isinstance(first_val, (datetime,)) or hasattr(first_val, 'isoformat'):
                    col_analysis["type"] = "datetime"
                    try:
                        date_values = [v for v in values if hasattr(v, 'isoformat') or isinstance(v, datetime)]
                        if date_values:
                            col_analysis.update({
                                "earliest": min(date_values).isoformat() if date_values else None,
                                "latest": max(date_values).isoformat() if date_values else None,
                                "date_range_days": (max(date_values) - min(date_values)).days if len(date_values) > 1 else 0
                            })
                    except Exception as e:
                        print(f"ë‚ ì§œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                        consistency_score -= 20
                        
                else:
                    col_analysis["type"] = "mixed"
                    col_analysis["data_quality_issues"].append("í˜¼í•©ëœ ë°ì´í„° íƒ€ì…")
                    consistency_score -= 25
            
            col_analysis["consistency_score"] = max(0, round(consistency_score, 1))
            analysis["columns"][col] = col_analysis
            
            total_completeness += completeness
            total_consistency += col_analysis["consistency_score"]
            
    except Exception as e:
        print(f"ë°ì´í„° êµ¬ì¡° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        analysis["patterns"].append(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # ì „ì²´ ë°ì´í„° í’ˆì§ˆ ê³„ì‚°
    num_columns = len(analysis["columns"])
    if num_columns > 0:
        analysis["data_quality"]["completeness"] = round(total_completeness / num_columns, 1)
        analysis["data_quality"]["consistency"] = round(total_consistency / num_columns, 1)
        analysis["data_quality"]["overall_score"] = round(
            (analysis["data_quality"]["completeness"] + analysis["data_quality"]["consistency"]) / 2, 1
        )
    
    return analysis

def generate_summary_insights(data_analysis: Dict, question: str = "") -> List[str]:
    """ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    insights = []
    
    # ë°ì´í„° í¬ê¸° ì¸ì‚¬ì´íŠ¸
    row_count = data_analysis["row_count"]
    if row_count > 10000:
        insights.append(f"ğŸ“Š **ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹**: {row_count:,}ê°œì˜ ë ˆì½”ë“œë¡œ êµ¬ì„±ëœ ìƒë‹¹í•œ ê·œëª¨ì˜ ë°ì´í„°ì…ë‹ˆë‹¤.")
    elif row_count > 1000:
        insights.append(f"ğŸ“Š **ì¤‘ê°„ ê·œëª¨ ë°ì´í„°ì…‹**: {row_count:,}ê°œì˜ ë ˆì½”ë“œë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    elif row_count < 10:
        insights.append(f"ğŸ“Š **ì†Œê·œëª¨ ë°ì´í„°ì…‹**: {row_count}ê°œì˜ ë ˆì½”ë“œë¡œ ì œí•œì ì¸ ìƒ˜í”Œì…ë‹ˆë‹¤.")
    
    # ë°ì´í„° í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸
    quality = data_analysis.get("data_quality", {})
    overall_score = quality.get("overall_score", 0)
    if overall_score >= 90:
        insights.append("âœ… **ë°ì´í„° í’ˆì§ˆ ìš°ìˆ˜**: ì™„ì„±ë„ì™€ ì¼ê´€ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.")
    elif overall_score >= 70:
        insights.append("âš ï¸ **ë°ì´í„° í’ˆì§ˆ ì–‘í˜¸**: ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•˜ë‚˜ ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    elif overall_score < 50:
        insights.append("ğŸš¨ **ë°ì´í„° í’ˆì§ˆ ì£¼ì˜**: ë°ì´í„° ì •ì œ ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ì»¬ëŸ¼ë³„ ì¸ì‚¬ì´íŠ¸
    columns = data_analysis.get("columns", {})
    numeric_columns = [col for col, stats in columns.items() if stats.get("type") == "numeric"]
    categorical_columns = [col for col, stats in columns.items() if stats.get("type") == "categorical"]
    
    if numeric_columns:
        insights.append(f"ğŸ”¢ **ìˆ«ìí˜• ë°ì´í„°**: {len(numeric_columns)}ê°œ ì»¬ëŸ¼ ({', '.join(numeric_columns[:3])}{'...' if len(numeric_columns) > 3 else ''})")
    
    if categorical_columns:
        insights.append(f"ğŸ“‚ **ë²”ì£¼í˜• ë°ì´í„°**: {len(categorical_columns)}ê°œ ì»¬ëŸ¼ ({', '.join(categorical_columns[:3])}{'...' if len(categorical_columns) > 3 else ''})")
    
    # íŠ¹ì • ì»¬ëŸ¼ì˜ ì£¼ëª©í•  ë§Œí•œ íŠ¹ì„±
    for col, stats in list(columns.items())[:3]:  # ìƒìœ„ 3ê°œ ì»¬ëŸ¼ë§Œ ë¶„ì„
        if stats["type"] == "numeric" and "sum" in stats:
            if stats["sum"] > 1000000:
                insights.append(f"ğŸ’° **{col}**: ì´í•© {stats['sum']:,.0f}ë¡œ ë†’ì€ ìˆ˜ì¹˜ë¥¼ ë³´ì…ë‹ˆë‹¤.")
            elif stats.get("range", 0) > stats.get("mean", 0) * 10:
                insights.append(f"ğŸ“ˆ **{col}**: ë„“ì€ ë¶„í¬ ë²”ìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤ (ìµœì†Œ: {stats.get('min', 0):,.0f}, ìµœëŒ€: {stats.get('max', 0):,.0f})")
        
        elif stats["type"] == "categorical":
            cardinality = stats.get("cardinality", 0)
            if cardinality < 10:
                insights.append(f"ğŸ·ï¸ **{col}**: ë‚®ì€ ì¹´ë””ë„ë¦¬í‹°({stats.get('unique_count', 0)}ê°œ ê³ ìœ ê°’)ë¡œ ê·¸ë£¹í™” ë¶„ì„ì— ì í•©í•©ë‹ˆë‹¤.")
            elif cardinality > 80:
                insights.append(f"ğŸŒŸ **{col}**: ë†’ì€ ì¹´ë””ë„ë¦¬í‹°({stats.get('unique_count', 0)}ê°œ ê³ ìœ ê°’)ë¡œ ì‹ë³„ì ì—­í• ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ì§ˆë¬¸ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
    if question:
        if any(keyword in question.lower() for keyword in ['trend', 'íŠ¸ë Œë“œ', 'time', 'ì‹œê°„', 'ë‚ ì§œ']):
            datetime_columns = [col for col, stats in columns.items() if stats.get("type") == "datetime"]
            if datetime_columns:
                insights.append(f"ğŸ“… **ì‹œê³„ì—´ ë¶„ì„ ê°€ëŠ¥**: {', '.join(datetime_columns)} ì»¬ëŸ¼ìœ¼ë¡œ ì‹œê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        if any(keyword in question.lower() for keyword in ['compare', 'ë¹„êµ', 'vs', 'ëŒ€ë¹„']):
            if len(categorical_columns) >= 2:
                insights.append(f"ğŸ”„ **ë¹„êµ ë¶„ì„ ì í•©**: ì—¬ëŸ¬ ë²”ì£¼í˜• ì»¬ëŸ¼ì„ í™œìš©í•œ ë¹„êµ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    return insights

def detect_column_relationships(data: List[Dict]) -> List[Dict]:
    """ì»¬ëŸ¼ ê°„ì˜ ì ì¬ì  ê´€ê³„ë¥¼ ê°ì§€"""
    if not data or len(data) < 2:
        return []
    
    relationships = []
    columns = list(data[0].keys())
    
    for i, col1 in enumerate(columns):
        for col2 in columns[i+1:]:
            try:
                # ê°’ë“¤ ì¶”ì¶œ
                values1 = [row.get(col1) for row in data if row.get(col1) is not None]
                values2 = [row.get(col2) for row in data if row.get(col2) is not None]
                
                if not values1 or not values2:
                    continue
                
                # ë™ì¼í•œ í–‰ì—ì„œ ê°’ë“¤ ì¶”ì¶œ
                paired_values = [(row.get(col1), row.get(col2)) for row in data 
                               if row.get(col1) is not None and row.get(col2) is not None]
                
                if len(paired_values) < 2:
                    continue
                
                # ê´€ê³„ ìœ í˜• ê°ì§€
                relationship_type = None
                confidence = 0
                
                # 1. ìˆ«ìí˜• ìƒê´€ê´€ê³„ (ë‘ ì»¬ëŸ¼ ëª¨ë‘ ìˆ«ìì¸ ê²½ìš°)
                if (isinstance(values1[0], (int, float)) and 
                    isinstance(values2[0], (int, float))):
                    
                    try:
                        import numpy as np
                        corr = np.corrcoef([v[0] for v in paired_values], 
                                          [v[1] for v in paired_values])[0, 1]
                        if abs(corr) > 0.7:
                            relationship_type = "strong_correlation"
                            confidence = abs(corr) * 100
                        elif abs(corr) > 0.3:
                            relationship_type = "moderate_correlation"  
                            confidence = abs(corr) * 100
                    except:
                        pass
                
                # 2. ì™¸ë˜í‚¤ ê´€ê³„ ì¶”ì •
                if not relationship_type:
                    # col1ì˜ ê°’ë“¤ì´ col2ì—ì„œ ë°˜ë³µë˜ëŠ”ì§€ í™•ì¸
                    unique_col1 = set(values1)
                    unique_col2 = set(values2)
                    
                    if len(unique_col1) < len(values1) * 0.8:  # col1ì´ ë°˜ë³µê°’ì´ ë§ìŒ
                        overlap = len(unique_col1.intersection(unique_col2))
                        if overlap > len(unique_col1) * 0.5:
                            relationship_type = "potential_foreign_key"
                            confidence = (overlap / len(unique_col1)) * 100
                
                # 3. ê³„ì¸µì  ê´€ê³„ (ëª…ëª… íŒ¨í„´ ê¸°ë°˜)
                if not relationship_type:
                    if (col1.lower().endswith('_id') and col2.lower().endswith('_name')) or \
                       (col1.lower().endswith('_code') and col2.lower().endswith('_description')):
                        relationship_type = "hierarchical"
                        confidence = 80
                
                if relationship_type and confidence > 30:
                    relationships.append({
                        "column1": col1,
                        "column2": col2,
                        "relationship_type": relationship_type,
                        "confidence": round(confidence, 1),
                        "description": _get_relationship_description(relationship_type, col1, col2)
                    })
                    
            except Exception as e:
                print(f"ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ({col1}, {col2}): {e}")
                continue
    
    return sorted(relationships, key=lambda x: x["confidence"], reverse=True)

def _get_relationship_description(rel_type: str, col1: str, col2: str) -> str:
    """ê´€ê³„ ìœ í˜•ì— ëŒ€í•œ ì„¤ëª… ìƒì„±"""
    descriptions = {
        "strong_correlation": f"{col1}ê³¼ {col2} ê°„ì— ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆìŠµë‹ˆë‹¤.",
        "moderate_correlation": f"{col1}ê³¼ {col2} ê°„ì— ì¤‘ê°„ ì •ë„ì˜ ìƒê´€ê´€ê³„ê°€ ìˆìŠµë‹ˆë‹¤.",
        "potential_foreign_key": f"{col1}ì´ {col2}ë¥¼ ì°¸ì¡°í•˜ëŠ” ì™¸ë˜í‚¤ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.",
        "hierarchical": f"{col1}ê³¼ {col2}ëŠ” ê³„ì¸µì  ê´€ê³„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    }
    return descriptions.get(rel_type, f"{col1}ê³¼ {col2} ê°„ì— ê´€ê³„ê°€ ìˆìŠµë‹ˆë‹¤.")

def format_data_for_visualization(data: List[Dict], chart_config: Dict) -> Dict:
    """ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë§ëŠ” í˜•íƒœë¡œ ë°ì´í„° í¬ë§·íŒ…"""
    if not data or not chart_config:
        return {}
    
    chart_type = chart_config.get("type", "bar")
    library = chart_config.get("chart_library", "Chart.js")
    
    if library == "Chart.js":
        return _format_for_chartjs(data, chart_config)
    else:
        return {"error": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬: {library}"}

def _format_for_chartjs(data: List[Dict], chart_config: Dict) -> Dict:
    """Chart.js í˜•íƒœë¡œ ë°ì´í„° í¬ë§·íŒ…"""
    chart_type = chart_config.get("type", "bar")
    label_col = chart_config.get("label_column")
    
    if chart_config.get("value_column"):
        # ë‹¨ì¼ ê°’ ì»¬ëŸ¼
        value_col = chart_config["value_column"]
        labels = [str(row.get(label_col, "")) for row in data]
        values = [row.get(value_col, 0) for row in data]
        
        return {
            "type": chart_type,
            "data": {
                "labels": labels,
                "datasets": [{
                    "label": value_col,
                    "data": values,
                    "backgroundColor": "rgba(66, 133, 244, 0.8)",
                    "borderColor": "rgba(66, 133, 244, 1)",
                    "borderWidth": 1
                }]
            },
            "options": {
                "responsive": True,
                "plugins": {
                    "title": {
                        "display": True,
                        "text": chart_config.get("title", "ë°ì´í„° ì°¨íŠ¸")
                    }
                }
            }
        }
    
    elif chart_config.get("value_columns"):
        # ë‹¤ì¤‘ ê°’ ì»¬ëŸ¼
        value_cols = chart_config["value_columns"]
        labels = [str(row.get(label_col, "")) for row in data]
        
        datasets = []
        colors = [
            "rgba(66, 133, 244, 0.8)",
            "rgba(52, 168, 83, 0.8)", 
            "rgba(251, 188, 5, 0.8)",
            "rgba(234, 67, 53, 0.8)",
            "rgba(142, 36, 170, 0.8)"
        ]
        
        for i, col in enumerate(value_cols):
            datasets.append({
                "label": col,
                "data": [row.get(col, 0) for row in data],
                "backgroundColor": colors[i % len(colors)],
                "borderColor": colors[i % len(colors)].replace("0.8", "1"),
                "borderWidth": 1
            })
        
        return {
            "type": chart_type,
            "data": {
                "labels": labels,
                "datasets": datasets
            },
            "options": {
                "responsive": True,
                "plugins": {
                    "title": {
                        "display": True,
                        "text": chart_config.get("title", "ë°ì´í„° ì°¨íŠ¸")
                    }
                }
            }
        }
    
    return {"error": "ì°¨íŠ¸ ì„¤ì •ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."}